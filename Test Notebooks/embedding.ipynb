{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape, Lambda\n",
    "from keras.layers import Input, Embedding, concatenate, Multiply\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import preprocessing\n",
    "from keras.regularizers import l2\n",
    "import random\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/Users/BharathiSrinivasan/Documents/GitHub/Thesis/merged_data.csv')\n",
    "folder = 'C:\\\\Users\\\\Pascal\\\\Documents\\\\GitHub\\\\instacart-market-basket-analysis\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.read_csv(folder + 'merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample orders of n customer\n",
    "def data_nusers(df, n):\n",
    "    unique_users = df.user_id.unique()\n",
    "    i = 0\n",
    "    df_nusers = pd.DataFrame()  \n",
    "    for user in unique_users:\n",
    "        df_nusers = df_nusers.append(df[df.user_id == user])\n",
    "        i +=1\n",
    "        if (i == n):\n",
    "            break\n",
    "    return pd.DataFrame(df_nusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = data_nusers(df_big,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_use.product_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of product IDs available\n",
    "N_products = df_use['product_id'].nunique()\n",
    "N_shoppers = df_use['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_COLUMNS = [\"user_id\", \"product_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper to index columns before embeddings\n",
    "def val2idx(df, cols):\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x]+1)\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796 10\n"
     ]
    }
   ],
   "source": [
    "print(N_products,N_shoppers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deep, unique_vals = val2idx(df_use, EMBEDDING_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10624, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_prod(order):\n",
    "    for _,row in order.iterrows():\n",
    "        if row['add_to_cart_order']==1:\n",
    "            return row['product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_prod(order):\n",
    "    for _,row in order.iterrows():\n",
    "        if row['add_to_cart_order']==2:\n",
    "            return row['product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basket(order):\n",
    "    order['product_id']= order['product_id'].astype(str)\n",
    "    \n",
    "    basket = []\n",
    "    for _,row in order.iterrows():\n",
    "        if row['add_to_cart_order']!=1:\n",
    "            basket.append(row['product_id'])\n",
    "    #basket = random.shuffle(basket)\n",
    "    return basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_embedding(df):\n",
    "    first = df.groupby(['order_id']).apply(first_prod)\n",
    "    next_product = df.groupby(['order_id']).apply(lambda x:next_prod(x))\n",
    "    basket =df.groupby(['order_id']).apply(lambda x: create_basket(x))\n",
    "    \n",
    "    transform_df = pd.DataFrame(first, columns = ['first_prod'])\n",
    "    transform_df['next_product']= next_product.values\n",
    "    transform_df['basket']= basket.tolist()\n",
    "    transform_df.reset_index(inplace=True)\n",
    "\n",
    "    # Number of product IDs available\n",
    "    N_products = df['product_id'].nunique()\n",
    "    N_shoppers = df['user_id'].nunique()\n",
    "\n",
    "    return transform_df, N_products, N_shoppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, N_products, N_shoppers = transform_data_for_embedding(df_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for _,row in final_df.iterrows():\n",
    "#    row.basket = random.shuffle(row.basket)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_for_embed_network(df, transform_df, N_products):\n",
    "\n",
    "    # Creating df with order_id, user_id, first prod, next prod, basket \n",
    " \n",
    "    print('next function', transform_df.head())\n",
    "    x = df.drop_duplicates(subset=['order_id','user_id'])\n",
    "    train_df = pd.merge(transform_df, x[['order_id','user_id']], how='left', on='order_id' )\n",
    "    train_df.dropna(inplace=True)\n",
    "\n",
    "    # Creating basket as categorical matrix for deep neural network output\n",
    "    names = []\n",
    "    for col in range(N_products):\n",
    "        names.append('col_' + str(col)) \n",
    "\n",
    "    basket_df = pd.DataFrame(columns= names)\n",
    "    for i,row in train_df.iterrows():\n",
    "        for val in row.basket:\n",
    "            if val!=0:\n",
    "                basket_df.loc[i,'col_'+val] = 1\n",
    "    basket_df.fillna(0, inplace=True)\n",
    "    basket_in.drop(['col_0'], axis=1, inplace=True)\n",
    "\n",
    "    train_df['next_product'] = train_df['next_product'].astype('category', categories = df.product_id.unique())\n",
    "    y_df = pd.get_dummies(train_df, columns = ['next_product'])\n",
    "    y_df.drop(['user_id','order_id','first_prod','basket'], axis=1, inplace=True)\n",
    "    \n",
    "    train_df.drop(['order_id','next_product','basket'], axis=1, inplace=True)\n",
    "\n",
    "    return train_df['first_prod'], train_df['user_id'], basket_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next function    order_id  first_prod  next_product  \\\n",
      "0         2           1           2.0   \n",
      "1         3          51          52.0   \n",
      "2         4         184         185.0   \n",
      "3         5         168         286.0   \n",
      "4         6         431         432.0   \n",
      "\n",
      "                                              basket  \n",
      "0                           [2, 3, 4, 5, 6, 7, 8, 9]  \n",
      "1                       [52, 53, 54, 55, 56, 57, 58]  \n",
      "2  [185, 186, 187, 188, 189, 190, 191, 192, 193, ...  \n",
      "3  [286, 287, 288, 199, 289, 290, 291, 292, 293, ...  \n",
      "4                                         [432, 433]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pascal\\Anaconda3\\envs\\Deep_Learning\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "product_in , user_in, basket_in, predicted_product = create_input_for_embed_network(df_deep, df1, N_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pascal\\Anaconda3\\envs\\Deep_Learning\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n"
     ]
    }
   ],
   "source": [
    "# Integer IDs representing 1-hot encodings\n",
    "prior_in = Input(shape=(1,))\n",
    "shopper_in = Input(shape=(1,))\n",
    "\n",
    "# Dense N-hot encoding for candidate products\n",
    "candidates_in = Input(shape=(N_products,))\n",
    "\n",
    "# Embeddings\n",
    "prior = Embedding(N_products+1, 10)(prior_in)\n",
    "shopper = Embedding(N_shoppers+1, 10)(shopper_in)\n",
    "\n",
    "# Reshape and merge all embeddings together\n",
    "reshape = Reshape(target_shape=(10,))\n",
    "combined = keras.layers.concatenate([reshape(prior), reshape(shopper)])\n",
    "\n",
    "# Hidden layers\n",
    "#hidden_1 = Dense(1024, activation='relu',W_regularizer=l2(0.02))(combined)\n",
    "#hidden_2 = Dense(512, activation='relu',W_regularizer=l2(0.02))(hidden_1)\n",
    "hidden_3 = Dense(100, activation='relu')(combined)\n",
    "#LR1 = LeakyReLU(alpha=0.1)(hidden_3)\n",
    "hidden_4 = Dense(1, activation='relu')(hidden_3)\n",
    "\n",
    "# Final 'fan-out' into the space of future products\n",
    "final = Dense(N_products, activation='relu')(hidden_4)\n",
    "#LR_final = LeakyReLU(alpha=0.1)(final)\n",
    "\n",
    "# Ensure we do not overflow when we exponentiate\n",
    "final = Lambda(lambda x: x - K.max(x))(final)\n",
    "\n",
    "# Masked soft-max using Lambda and merge-multiplication\n",
    "exponentiate = Lambda(lambda x: K.exp(x))(final)\n",
    "masked = keras.layers.multiply([exponentiate, candidates_in])\n",
    "predicted = Lambda(lambda x: x / K.sum(x))(masked)\n",
    "\n",
    "# Compile with categorical crossentropy and adam\n",
    "mdl = Model(input=[prior_in , shopper_in, candidates_in],\n",
    "            output=predicted)\n",
    "mdl.compile(loss='categorical_crossentropy', \n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_46 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_31 (Embedding)        (None, 1, 10)        22520       input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_32 (Embedding)        (None, 1, 10)        310         input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 10)           0           embedding_31[0][0]               \n",
      "                                                                 embedding_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 20)           0           reshape_16[0][0]                 \n",
      "                                                                 reshape_16[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 100)          2100        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            101         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 2251)         4502        dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 2251)         0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 2251)         0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, 2251)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 2251)         0           lambda_10[0][0]                  \n",
      "                                                                 input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 2251)         0           multiply_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 29,533\n",
      "Trainable params: 29,533\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f042a7ef98>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.fit([product_in , user_in, basket_in], predicted_product,  batch_size=128, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = mdl.to_json()\n",
    "with open(\"NN_embed_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "mdl.save_weights(\"NN_embed_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
