{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/BharathiSrinivasan/Documents/GitHub/Thesis/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['prd_reorder_freq'], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"order_dow\", \"order_hour_of_day\",\"aisle_id\",\"department_id\"]\n",
    "CONTINUOUS_COLUMNS = [\"days_since_prior_order\",\"order_number\",\"add_to_cart_order\", \\\n",
    "                     \"user_period\",\"user_distinct_products\", \\\n",
    "                     \"user_average_basket\",\"product_id_orders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding categorical columns\n",
    "df = pd.get_dummies(df, columns=[x for x in CATEGORICAL_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising the feature columns\n",
    "df[CONTINUOUS_COLUMNS] = preprocessing.MinMaxScaler().fit_transform(df[CONTINUOUS_COLUMNS].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['product_name','department','product_id','user_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df.reordered.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['reordered'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.20, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype = theano.config.floatX)\n",
    "y_train = np.array(y_train, dtype = theano.config.floatX)\n",
    "X_test = np.array(X_test, dtype = theano.config.floatX)\n",
    "y_test = np.array(y_test, dtype = theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_nn(ann_input, ann_output):\n",
    "\n",
    "  n_hidden = 100\n",
    "    \n",
    "    # Initialize random weights between each layer\n",
    "  init_1 = np.random.randn(df.shape[1], n_hidden).astype(theano.config.floatX)\n",
    "  init_2 = np.random.randn(n_hidden, n_hidden).astype(theano.config.floatX)\n",
    "  init_3 = np.random.randn(n_hidden, n_hidden).astype(theano.config.floatX)\n",
    "  init_out = np.random.randn(n_hidden).astype(theano.config.floatX)\n",
    "  \n",
    "  with pm.Model() as neural_network:\n",
    "        # Weights from input to hidden layer\n",
    "        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n",
    "                                 shape=(df.shape[1], n_hidden), \n",
    "                                 testval=init_1)\n",
    "        \n",
    "        # Weights from 1st to 2nd layer\n",
    "        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n",
    "                                shape=(n_hidden, n_hidden), \n",
    "                                testval=init_2)\n",
    "        \n",
    "        # Weights from 2nd to 3rd layer\n",
    "        weights_2_3 = pm.Normal('w_2_3', 0, sd=1, \n",
    "                                shape=(n_hidden, n_hidden), \n",
    "                                testval=init_3)\n",
    "        \n",
    "        # Weights from hidden layer to output\n",
    "        weights_3_out = pm.Normal('w_3_out', 0, sd=1, \n",
    "                                  shape=(n_hidden,), \n",
    "                                  testval=init_out)\n",
    "        \n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n",
    "                                         weights_in_1))\n",
    "        act_2 = pm.math.tanh(pm.math.dot(act_1, \n",
    "                                         weights_1_2))\n",
    "        act_3 = pm.math.tanh(pm.math.dot(act_2, \n",
    "                                         weights_2_3))\n",
    "        act_out = pm.math.dot(act_3, weights_3_out)\n",
    "        \n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Normal('out', \n",
    "                           act_out,\n",
    "                           observed=ann_output,\n",
    "                           total_size=y_train.shape[0] # IMPORTANT for minibatches\n",
    "                          )\n",
    "        return neural_network\n",
    "\n",
    "  \n",
    "# Trick: Turn inputs and outputs into shared variables. \n",
    "# It's still the same thing, but we can later change the values of the shared variable \n",
    "# (to switch in the test-data later) and pymc3 will just use the new data. \n",
    "# Kind-of like a pointer we can redirect.\n",
    "# For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "ann_input = theano.shared(X_train)\n",
    "ann_output = theano.shared(y_train)\n",
    "neural_network = construct_nn(ann_input, ann_output)\n",
    "\n",
    "# Usually we would add a constant b to the inputs but I omitted it here to keep the code cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3.theanof import set_tt_rng, MRG_RandomStreams\n",
    "set_tt_rng(MRG_RandomStreams(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with neural_network:\n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=10, method=inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = approx.sample(draws=1000)\n",
    "\n",
    "plt.plot(-inference.hist)\n",
    "plt.ylabel('ELBO')\n",
    "plt.xlabel('iteration');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace arrays our NN references with the test data\n",
    "ann_input.set_value(X_test)\n",
    "ann_output.set_value(y_test)\n",
    "\n",
    "with neural_network:\n",
    "    ppc = pm.sample_ppc(trace, samples=200, progressbar=False)\n",
    "\n",
    "# Use probability of > 0.5 to assume prediction of class 1\n",
    "pred = ppc['out'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
